# Chapter 1 Introduction

P. A. Vanro/leghem, J. B. Copp, K. V. Gernaey and U. Jeppsson

This Scientific and Technical Report (STR) is the summary of the work of the IWA Task Group on Benchmarking of Control Strategies for Wastewater Treatment Plants. As will be explained in Chapter 2. this Task Group has a long history. However, before describing this history and the results of the Task Group in more detail, we would first like to use this introduction to provide more insight into a number of basic issues related to the family of Benchmark Simulation Models (BSMs), which are the main 'products' of this Task Group. In order to do this, we will seek answers to a few basic questions: What is meant by a 'Benchmark Simulation Model'? What is the purpose of the BSMs? Who should use the BSMs? How should a BSM be used? Who has been involved in the development of the BSMs? And last but not least, how should this STR be read?

##   1.1 WHAT IS MEANT BY A 'BENCHMARK SIMULATION MODEL'?

When checking a dictionary, a benchmark is defined as a measure of reference to be used in a test. In computer science a benchmark is a reference performance to which the relative performance of hardware or software can be compared. In process modelling and control, a benchmark is defined as a plant model and associated control strategy that can be used as a reference point for simulation-based comparison of control strategies (Downs & Vogel, 1993). Such a simulation benchmark is not associated with a particular simulation platform. Direct coding (e.g., C/C++, FORTRAN) as well as commercial simulation software packages can be used. In this case, the purpose of the simulation protocol was to create a tool that could guarantee that different users obtain exactly the same results when running the simulation model. The main 'products' of this Task Group are WWTP simulation models (Chapter 4), a simulation protocol for these simulation models (Chapter 7) and a set of benchmarking evaluation criteria for objective control evaluation (Chapter 6). All these items together form the benchmark simulation model platform. However, it should be emphasised as well that a major result of the Task Group work is a set of verified unit process models and tools that are applicable to WWTP simulation studies in general. The Activated Sludge Model No. I (ASMI), Tak√•cs secondary clarifier model and Anaerobic Digestion Model No. I (ADM l) among others were all verified before including those unit process models in the BSM platform. More details on these models will be provided in Chapter 4. Other tools presented in Chapter 4, such as a dynamic influent pollution disturbance scenario generator (DIPDSG) model, AS-AD interface models, (fault) models and a risk assessment module for microbiology related settling problems from the work of the Task Group.

## 1.2 WHAT IS THE PURPOSE OF THE BENCHMARK slMULATlON

The activated sludge process aims to achieve, at minimum cost, sufficiently low concentration of biodegradable matter and nutrients in the effluent together with minimal sludge production. To do this, the process has to be controlled. Many control strategies have been proposed in the literature; however, the literature does not provide a clear basis for comparison of these strategies because of the many influences that have an impact on the system. Many of these influences are easily recognised. For instance, physical characteristics of the process can have an impact on process performance and this makes the comparison of strategies applied to different reactor layouts difficult. As well, the influence ()f a control strategy on process performance is expected to vary with different disturbances, thus the disturbances used to test the control strategy become important. Also complicating the evaluation is the lack of standard evaluation criteria. Effluent requirements are for example often location specific, which make it difficult to judge the particular influence of an applied control strategy from a reported performance increase. Controversies that result from the resulting subjective reports reinforce the need to devise an effective and unbiased evaluation method that can be used to judge the utility of different control strategies.

From a practical standpoint, it is not reasonable to experimentally test and verify the effectiveness of  all reported control strategies and often the assessment of these control strategies is confounded by the multi-faceted nature of the process under study. Alternatively, given a standardised procedure, it is possible to efficiently evaluate numerous strategies through realistic/dynamic computer simulations. Simulations provide a cost-effective means for the evaluation of control strategies, but the unlimited number of simulation permutations makes the need for a standardised protocol very important if different strategies (and different simulation results) are to be compared. Each control strategy must be simulated under the same conditions to ensure unbiased comparisons. Validation of the computer simulations is difficult without supporting experimental or full-scale data, but the value of the work is enhanced through the use of accepted activated sludge models. Because appropriate simulation tools for the activated sludge are available this approach has numerous advantages, but still there is a need for a standardised procedure. To this end, there has been an effort to develop a standardised simulation protocol - a 'simulation benchmark'

The BSM platform was originally intended exclusively for the simulation-based comparison of WWTP control strategies. This was later extended to include the comparison of WWTP monitoring strategies as well (Chapters 4, 5, 6 and 7). Process monitoring is the activity by which collected data are analysed to find process deviations, failures and faults. Process monitoring also involves isolation of variables contributing to the deviations, facilitating further analysis of the problems. Again, many monitoring methods have been  proposed in the literature but an unbiased evaluation of their performance in a WWTP was not available  even though it was highly desired. Consequently, a standardised simulation protocol, verified models and  evaluation criteria were established to address this deficiency.

## 1.3 WHO SHOULD USE THE BENCHMARK SIMULATION MODELS?
It is not that easy to describe a specific target group for the BSM platform. In fact, the models and tools described in this STR should appeal to a broad audience - that audience will be called 'benchmark users' in the rest of the STR - both from industry and academia with a general interest in WWTP modelling. This is best illustrated with a few examples. The benchmark simulation models can be used for objective simulation-based comparison of control strategies. As such, users from academia might find it interesting to develop a control strategy that yield the best overall performance. They may also use it to train their students on the basics of process control. A consultant, on the other hand, might use these models to demonstrate the use of control strategies to a potential customer. A benchmark user might also be interested in using only part of the benchmark simulation model(e.g., the evaluation criteria, or sensor and actuator models, or the influent generator). And of course, individual unit process models can be used in any simulation study as they all are now verified by the Task Group and distributed for free when requested.

## 1.4 HOW SHOULD THE BENCHMARK SIMULATION MODELS BE USED?
The BSM platform is a standardised simulation protocol. If it is the intention of the benchmark user to work with the BSM platform and to compare simulation results with other benchmark users, then the standardised simulation protocol should be followed. A space limitation in this STR means that it was not possible to provide a description of all the minute details of the BSMs. Rather, this publication provides a detailed overview of the BSMs and the various tools. For more specific details, the reader is referred to the detailed technical reports and BSM computer code which accompany the STR.

The benchmark user is of course free to modify the models and tools provided by the Task Group. However, such modifications should be mentioned clearly and documented properly when publishing results in order to allow a fair comparison of results.

## 1.5 WHO HAS BEEN INVOLVED IN THE DEVELOPMENT OF THE BENCHMARK SIMULATION MODELS?

As will be explained in Chapter 2 it has taken several years to develop the BSM platform and the Task Group 'products' are truly the result of a group effort. Over the years, many people have contributed to the BSM platform development. These people were not all part of the Task Group and many did not specifically contribute to this publication. However, those individuals should be fully acknowledged for their work and input. The Task Group has tried to do so in the preface to this publication, knowing that we will probably never be able to name all the people that have been involved in the development over the years. In the rest of this STR, we will collectively refer to the BSM developers as the 'benchmarkers'.

## 1.6 HOW SHOULD THIS SCIENTIFIC AND TECHNICAL REPORT BE READ?

This STR is only a summary of the work of the Task Group and can be used to get a rapid overview of the results. The detailed technical reports were written by the benchmarkers while developing, implementing and validating the specific models and tools, and are as such the most detailed information provided by the Task Group. To supplement that reading, the reader can consult the model code provided by the Task Group in order to get a deeper understanding of the more technical issues. Finally, a benchmark user can find inspiration in the literature, as a significant number of scientific papers published during the years have been devoted to the use of the BSMs. These papers are listed in Technical Report No. 15. As a courtesy to the original contributors, benchmark users are asked to refer to those literature sources when publishing their own work, in order to properly acknowledge the work done by the different benchmarkers.
